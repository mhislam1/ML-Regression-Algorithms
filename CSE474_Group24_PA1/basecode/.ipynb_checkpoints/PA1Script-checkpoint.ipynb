{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE474/574 - Programming Assignment 1\n",
    "\n",
    "For grading, we will execute the submitted notebook as follows:\n",
    "\n",
    "```shell\n",
    "jupyter nbconvert --to python PA1Script.ipynb\n",
    "python PA1Script.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 - Linear Regression with Direct Minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEM 1\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print('PROBLEM 1')\n",
    "print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learnOLERegression(X,y):\n",
    "    # Inputs:                                                         \n",
    "    # X = N x d \n",
    "    # y = N x 1                                                               \n",
    "    # Output: \n",
    "    # w = d x 1 \n",
    "\n",
    "    # IMPLEMENT THIS METHOD - REMOVE THE NEXT LINE\n",
    "    # w = np.zeros((X.shape[0],1))\n",
    "    XT = X.transpose()\n",
    "    XTX_Inverse = np.linalg.inv(np.dot(XT,X)) # Source: https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.inv.html\n",
    "                                              # Source: https://www.tutorialspoint.com/numpy/numpy_inv.htm\n",
    "    XTX_InverseXT = np.dot(XTX_Inverse,XT)\n",
    "    w = np.dot(XTX_InverseXT,y)\n",
    "    return w "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testOLERegression(w,Xtest,ytest):\n",
    "    # Inputs:\n",
    "    # w = d x 1\n",
    "    # Xtest = N x d\n",
    "    # ytest = N x 1\n",
    "    # Output:\n",
    "    # rmse = scalar value\n",
    "    \n",
    "    # IMPLEMENT THIS METHOD - REMOVE THE NEXT LINE\n",
    "    rmse = 0\n",
    "    N = np.size(ytest)                            # To get N. \n",
    "                                                  # Source: https://docs.scipy.org/doc/numpy/reference/generated/numpy.ma.size.html\n",
    "    \n",
    "    \n",
    "    matrix = (ytest - np.dot(Xtest,w))\n",
    "    matrix_transpose = matrix.transpose()\n",
    "    dot_product = np.dot(matrix_transpose, matrix)\n",
    "    rmse = (dot_product/N)**(0.5)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE without intercept on train data - 138.20\n",
      "RMSE with intercept on train data - 46.77\n",
      "RMSE without intercept on test data - 326.76\n",
      "RMSE with intercept on test data - 60.89\n"
     ]
    }
   ],
   "source": [
    "Xtrain,ytrain,Xtest,ytest = pickle.load(open('diabetes.pickle','rb'),encoding='latin1')   \n",
    "# add intercept\n",
    "x1 = np.ones((len(Xtrain),1))\n",
    "x2 = np.ones((len(Xtest),1))\n",
    "\n",
    "Xtrain_i = np.concatenate((np.ones((Xtrain.shape[0],1)), Xtrain), axis=1)\n",
    "Xtest_i = np.concatenate((np.ones((Xtest.shape[0],1)), Xtest), axis=1)\n",
    "\n",
    "w = learnOLERegression(Xtrain,ytrain)\n",
    "w_i = learnOLERegression(Xtrain_i,ytrain)\n",
    "\n",
    "rmse = testOLERegression(w,Xtrain,ytrain)\n",
    "rmse_i = testOLERegression(w_i,Xtrain_i,ytrain)\n",
    "print('RMSE without intercept on train data - %.2f'%rmse)\n",
    "print('RMSE with intercept on train data - %.2f'%rmse_i)\n",
    "\n",
    "rmse = testOLERegression(w,Xtest,ytest)\n",
    "rmse_i = testOLERegression(w_i,Xtest_i,ytest)\n",
    "print('RMSE without intercept on test data - %.2f'%rmse)\n",
    "print('RMSE with intercept on test data - %.2f'%rmse_i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2 - Linear Regression with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEM 2\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print('PROBLEM 2')\n",
    "print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressionObjVal(w, X, y):\n",
    "\n",
    "    # compute squared error (scalar) with respect\n",
    "    # to w (vector) for the given data X and y      \n",
    "    #\n",
    "    # Inputs:\n",
    "    # w = d x 1\n",
    "    # X = N x d\n",
    "    # y = N x 1\n",
    "    # Output:\n",
    "    # error = scalar value\n",
    "\n",
    "    # IMPLEMENT THIS METHOD - REMOVE THE NEXT LINE\n",
    "    error = 0\n",
    "    w = w.reshape(w.shape[0],1)\n",
    "    Xw = np.dot(X,w)\n",
    "    matrix = (y - Xw)\n",
    "    product = (0.5)*matrix.transpose()\n",
    "    error = np.dot(product, matrix)\n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressionGradient(w, X, y):\n",
    "\n",
    "    # compute gradient of squared error (scalar) with respect\n",
    "    # to w (vector) for the given data X and y   \n",
    "    \n",
    "    # Inputs:\n",
    "    # w = d x 1\n",
    "    # X = N x d\n",
    "    # y = N x 1\n",
    "    # Output:\n",
    "    # gradient = d length vector (not a d x 1 matrix)\n",
    "\n",
    "    # IMPLEMENT THIS METHOD - REMOVE THE NEXT LINE \n",
    "    # print(w.shape)\n",
    "    w = w.reshape(w.shape[0],1)\n",
    "    XT = X.transpose()\n",
    "    Xw = np.dot(X,w)\n",
    "    XTy = np.dot(XT,y)\n",
    "    error_grad = np.dot(XT,Xw) - XTy          # d x 1 matrix\n",
    "    error_grad = error_grad.flatten()\n",
    "    # print(error_grad.shape)\n",
    "    return error_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent Linear Regression RMSE on train data - 48.06\n",
      "Gradient Descent Linear Regression RMSE on test data - 54.71\n"
     ]
    }
   ],
   "source": [
    "Xtrain,ytrain,Xtest,ytest = pickle.load(open('diabetes.pickle','rb'),encoding='latin1')   \n",
    "# add intercept\n",
    "Xtrain_i = np.concatenate((np.ones((Xtrain.shape[0],1)), Xtrain), axis=1)\n",
    "Xtest_i = np.concatenate((np.ones((Xtest.shape[0],1)), Xtest), axis=1)\n",
    "args = (Xtrain_i,ytrain)\n",
    "opts = {'maxiter' : 50}    # Preferred value.    \n",
    "w_init = np.zeros((Xtrain_i.shape[1],1))\n",
    "soln = minimize(regressionObjVal, w_init, jac=regressionGradient, args=args,method='CG', options=opts)\n",
    "w = np.transpose(np.array(soln.x))\n",
    "w = w[:,np.newaxis]\n",
    "rmse = testOLERegression(w,Xtrain_i,ytrain)\n",
    "print('Gradient Descent Linear Regression RMSE on train data - %.2f'%rmse)\n",
    "rmse = testOLERegression(w,Xtest_i,ytest)\n",
    "print('Gradient Descent Linear Regression RMSE on test data - %.2f'%rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Linear Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3 - Perceptron using Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEM 3\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print('PROBLEM 3')\n",
    "print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictLinearModel(w,Xtest):\n",
    "    # Inputs:\n",
    "    # w = d x 1\n",
    "    # Xtest = N x d\n",
    "    # Output:\n",
    "    # ypred = N x 1 vector of predictions\n",
    "\n",
    "    # IMPLEMENT THIS METHOD - REMOVE THE NEXT LINE\n",
    "    #ypred = np.zeros([Xtest.shape[0],1])\n",
    "    X_dot = np.dot(Xtest, w)\n",
    "    transpose = X_dot.transpose()\n",
    "    array = np.squeeze(np.asarray(transpose))\n",
    "    N = X_dot.shape[0]\n",
    "    counter = 0\n",
    "    while counter < N:\n",
    "        if array[counter] < 0:\n",
    "            array[counter] = -1\n",
    "        else:\n",
    "            array[counter] = 1\n",
    "        counter = counter + 1\n",
    "    ypred = X_dot\n",
    "    return ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateLinearModel(w,Xtest,ytest):\n",
    "    # Inputs:\n",
    "    # w = d x 1\n",
    "    # Xtest = N x d\n",
    "    # ytest = N x 1\n",
    "    # Output:\n",
    "    # acc = scalar values\n",
    "    \n",
    "\n",
    "    # IMPLEMENT THIS METHOD - REMOVE THE NEXT LINE\n",
    "    predicted = predictLinearModel(w, Xtest)\n",
    "    N = predicted.shape[0]\n",
    "    i = 0\n",
    "    counter = 0\n",
    "    while counter < N:\n",
    "        if predicted[counter] == ytest[counter]:\n",
    "            i = i + 1\n",
    "        counter = counter + 1    \n",
    "    acc = i / N\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron Accuracy on train data - 0.84\n",
      "Perceptron Accuracy on test data - 0.84\n"
     ]
    }
   ],
   "source": [
    "Xtrain,ytrain, Xtest, ytest = pickle.load(open('sample.pickle','rb')) \n",
    "# add intercept\n",
    "Xtrain_i = np.concatenate((np.ones((Xtrain.shape[0],1)), Xtrain), axis=1)\n",
    "Xtest_i = np.concatenate((np.ones((Xtest.shape[0],1)), Xtest), axis=1)\n",
    "\n",
    "args = (Xtrain_i,ytrain)\n",
    "opts = {'maxiter' : 50}    # Preferred value.    \n",
    "w_init = np.zeros((Xtrain_i.shape[1],1))\n",
    "soln = minimize(regressionObjVal, w_init, jac=regressionGradient, args=args,method='CG', options=opts)\n",
    "w = np.transpose(np.array(soln.x))\n",
    "w = w[:,np.newaxis]\n",
    "acc = evaluateLinearModel(w,Xtrain_i,ytrain)\n",
    "print('Perceptron Accuracy on train data - %.2f'%acc)\n",
    "acc = evaluateLinearModel(w,Xtest_i,ytest)\n",
    "print('Perceptron Accuracy on test data - %.2f'%acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4 - Logistic Regression Using Newton's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEM 4\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print('PROBLEM 4')\n",
    "print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticObjVal(w, X, y):\n",
    "\n",
    "    # compute log-loss error (scalar) with respect\n",
    "    # to w (vector) for the given data X and y                               \n",
    "    # Inputs:\n",
    "    # w = d x 1\n",
    "    # X = N x d\n",
    "    # y = N x 1\n",
    "    # Output:\n",
    "    # error = scalar\n",
    "    \n",
    "    \n",
    "    if len(w.shape) == 1:\n",
    "        w = w[:,np.newaxis]\n",
    "    # IMPLEMENT THIS METHOD - REMOVE THE NEXT LINE\n",
    "    error = 0\n",
    "    w = w.reshape(w.shape[0],1)\n",
    "    wTX = np.dot(X,w)\n",
    "    e = np.exp(-1 * wTX)\n",
    "    error = 1/(1 + e)\n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient shape is: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.011699  , -0.03303344,  0.09707363])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def logisticGradient(w, X, y):\n",
    "\n",
    "    # compute the gradient of the log-loss error (vector) with respect\n",
    "    # to w (vector) for the given data X and y  \n",
    "    #\n",
    "    # Inputs:\n",
    "    # w = d x 1\n",
    "    # X = N x d\n",
    "    # y = N x 1\n",
    "    # Output:\n",
    "    # error = d length gradient vector (not a d x 1 matrix)\n",
    "\n",
    "    if len(w.shape) == 1:\n",
    "        w = w[:,np.newaxis]\n",
    "    # IMPLEMENT THIS METHOD - REMOVE THE NEXT LINE\n",
    "    gradient = np.zeros((w.shape[0],))\n",
    "    #print('gradient shape is: ' + str(gradient.shape[0]) + ', ' + str(gradient.shape[1]))\n",
    "    w = w.reshape(w.shape[0],1)\n",
    "    wTX = np.dot(X,w) \n",
    "    ywTX = y*wTX\n",
    "    e = np.exp(ywTX)\n",
    "    denominator = 1 + e\n",
    "    gradient = (y/denominator)*X            #(1 x100)(100 x 3)\n",
    "    gradient = np.mean(gradient, axis=0)\n",
    "    gradient = gradient*-1\n",
    "    print('gradient shape is: ' + str(gradient.shape[0]))\n",
    "    gradient = gradient.flatten()\n",
    "    return gradient\n",
    "wtest = np.ndarray((3,1))\n",
    "xtest = np.ndarray((100, 3))\n",
    "ytest = np.ndarray((100, 1))\n",
    "\n",
    "logisticGradient(wtest, xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticHessian(w, X, y):\n",
    "\n",
    "    # compute the Hessian of the log-loss error (matrix) with respect\n",
    "    # to w (vector) for the given data X and y                               \n",
    "    #\n",
    "    # Inputs:\n",
    "    # w = d x 1\n",
    "    # X = N x d\n",
    "    # y = N x 1\n",
    "    # Output:\n",
    "    # Hessian = d x d matrix\n",
    "    \n",
    "    if len(w.shape) == 1:\n",
    "        w = w[:,np.newaxis]\n",
    "    # IMPLEMENT THIS METHOD - REMOVE THE NEXT LINE\n",
    "    hessian = np.eye(X.shape[1])\n",
    "    w = w.reshape(w.shape[0],1)\n",
    "    \n",
    "    wTX = np.dot(X,w)  #(N x d)(d x 1) = (N x 1)\n",
    "    #ywTX = np.dot(y, wTX)                   #(N x 1)(N x 1)\n",
    "    #e = np.exp(ywTX)\n",
    "    #denominator = (1 + e)**2\n",
    "    #fraction = e / denominator\n",
    "    #hessian = wTX\n",
    "    \n",
    "    return hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-160-b9b85bf438fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'maxiter'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m}\u001b[0m    \u001b[0;31m# Preferred value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mw_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0msoln\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogisticObjVal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogisticGradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogisticHessian\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Newton-CG'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoln\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'newton-cg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m         return _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m--> 607\u001b[0;31m                                   **options)\n\u001b[0m\u001b[1;32m    608\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m_minimize_newtoncg\u001b[0;34m(fun, x0, args, jac, hess, hessp, callback, xtol, eps, maxiter, disp, return_all, **unknown_options)\u001b[0m\n\u001b[1;32m   1652\u001b[0m             \u001b[0malphak\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_fval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_old_fval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgfkp1\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1653\u001b[0m                      _line_search_wolfe12(f, fprime, xk, pk, gfk,\n\u001b[0;32m-> 1654\u001b[0;31m                                           old_fval, old_old_fval)\n\u001b[0m\u001b[1;32m   1655\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_LineSearchError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0;31m# Line search failed to find a better solution.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m_line_search_wolfe12\u001b[0;34m(f, fprime, xk, pk, gfk, old_fval, old_old_fval, **kwargs)\u001b[0m\n\u001b[1;32m    862\u001b[0m                                      \u001b[0mold_fval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_old_fval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m                                      \u001b[0mextra_condition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextra_condition\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 864\u001b[0;31m                                      **kwargs2)\n\u001b[0m\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/scipy/optimize/linesearch.py\u001b[0m in \u001b[0;36mline_search_wolfe2\u001b[0;34m(f, myfprime, xk, pk, gfk, old_fval, old_old_fval, args, c1, c2, amax, extra_condition, maxiter)\u001b[0m\n\u001b[1;32m    309\u001b[0m     alpha_star, phi_star, old_fval, derphi_star = scalar_search_wolfe2(\n\u001b[1;32m    310\u001b[0m             \u001b[0mphi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mderphi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_fval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_old_fval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mderphi0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamax\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m             extra_condition2, maxiter=maxiter)\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mderphi_star\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/scipy/optimize/linesearch.py\u001b[0m in \u001b[0;36mscalar_search_wolfe2\u001b[0;34m(phi, derphi, phi0, old_phi0, derphi0, c1, c2, amax, extra_condition, maxiter)\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mphi_a1\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mphi0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mc1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0malpha1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mderphi0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m            \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphi_a1\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mphi_a0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0malpha_star\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi_star\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mderphi_star\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "Xtrain,ytrain, Xtest, ytest = pickle.load(open('sample.pickle','rb')) \n",
    "# add intercept\n",
    "Xtrain_i = np.concatenate((np.ones((Xtrain.shape[0],1)), Xtrain), axis=1)\n",
    "Xtest_i = np.concatenate((np.ones((Xtest.shape[0],1)), Xtest), axis=1)\n",
    "\n",
    "args = (Xtrain_i,ytrain)\n",
    "opts = {'maxiter' : 50}    # Preferred value.    \n",
    "w_init = np.zeros((Xtrain_i.shape[1],1))\n",
    "soln = minimize(logisticObjVal, w_init, jac=logisticGradient, hess=logisticHessian, args=args,method='Newton-CG', options=opts)\n",
    "w = np.transpose(np.array(soln.x))\n",
    "w = np.reshape(w,[len(w),1])\n",
    "acc = evaluateLinearModel(w,Xtrain_i,ytrain)\n",
    "print('Logistic Regression Accuracy on train data - %.2f'%acc)\n",
    "acc = evaluateLinearModel(w,Xtest_i,ytest)\n",
    "print('Logistic Regression Accuracy on test data - %.2f'%acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5 - Support Vector Machines Using Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBLEM 5\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print('PROBLEM 5')\n",
    "print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainSGDSVM(X,y,T,eta=0.01):\n",
    "    # learn a linear SVM by implementing the SGD algorithm\n",
    "    #\n",
    "    # Inputs:\n",
    "    # X = N x d\n",
    "    # y = N x 1\n",
    "    # T = number of iterations\n",
    "    # eta = learning rate\n",
    "    # Output:\n",
    "    # weight vector, w = d x 1\n",
    "    \n",
    "    # IMPLEMENT THIS METHOD\n",
    "    w = np.zeros([X.shape[1],1])\n",
    "    N = X.shape[0]\n",
    "    w_transpose = w.transpose()\n",
    "    \n",
    "    iterations = range(1, T)\n",
    "    for i in iterations:\n",
    "        j = (np.random.randint(1, N, 1))[0]\n",
    "        yj = y.item(j - 1)\n",
    "        xj = X.item(j - 1)\n",
    "        prod = np.dot(np.dot(yj, w_transpose), xj)\n",
    "        prodVal = prod.item(0)\n",
    "        if prodVal < 1:\n",
    "            w = w + (eta * np.dot(yj, xj))\n",
    "            \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy on train data - 0.49\n",
      "SVM Accuracy on test data - 0.63\n"
     ]
    }
   ],
   "source": [
    "Xtrain,ytrain, Xtest, ytest = pickle.load(open('sample.pickle','rb')) \n",
    "# add intercept\n",
    "Xtrain_i = np.concatenate((np.ones((Xtrain.shape[0],1)), Xtrain), axis=1)\n",
    "Xtest_i = np.concatenate((np.ones((Xtest.shape[0],1)), Xtest), axis=1)\n",
    "\n",
    "args = (Xtrain_i,ytrain)\n",
    "w = trainSGDSVM(Xtrain_i,ytrain,100,0.01)\n",
    "acc = evaluateLinearModel(w,Xtrain_i,ytrain)\n",
    "print('SVM Accuracy on train data - %.2f'%acc)\n",
    "acc = evaluateLinearModel(w,Xtest_i,ytest)\n",
    "print('SVM Accuracy on test data - %.2f'%acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6 - Plotting decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Problem 6')\n",
    "print('---------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotBoundaries(w,X,y):\n",
    "    # plotting boundaries\n",
    "\n",
    "    mn = np.min(X,axis=0)\n",
    "    mx = np.max(X,axis=0)\n",
    "    x1 = np.linspace(mn[1],mx[1],100)\n",
    "    x2 = np.linspace(mn[2],mx[2],100)\n",
    "    xx1,xx2 = np.meshgrid(x1,x2)\n",
    "    xx = np.zeros((x1.shape[0]*x2.shape[0],2))\n",
    "    xx[:,0] = xx1.ravel()\n",
    "    xx[:,1] = xx2.ravel()\n",
    "    xx_i = np.concatenate((np.ones((xx.shape[0],1)), xx), axis=1)\n",
    "    ypred = predictLinearModel(w,xx_i)\n",
    "    ax.contourf(x1,x2,ypred.reshape((x1.shape[0],x2.shape[0])),alpha=0.3,cmap='cool')\n",
    "    ax.scatter(X[:,1],X[:,2],c=y.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain,ytrain, Xtest, ytest = pickle.load(open('sample.pickle','rb')) \n",
    "# add intercept\n",
    "Xtrain_i = np.concatenate((np.ones((Xtrain.shape[0],1)), Xtrain), axis=1)\n",
    "Xtest_i = np.concatenate((np.ones((Xtest.shape[0],1)), Xtest), axis=1)\n",
    "\n",
    "# Replace next three lines with code for learning w using the three methods\n",
    "w_perceptron = np.zeros((Xtrain_i.shape[1],1))\n",
    "w_logistic = np.zeros((Xtrain_i.shape[1],1))\n",
    "w_svm = np.zeros((Xtrain_i.shape[1],1))\n",
    "fig = plt.figure(figsize=(20,6))\n",
    "\n",
    "ax = plt.subplot(1,3,1)\n",
    "plotBoundaries(w_perceptron,Xtrain_i,ytrain)\n",
    "ax.set_title('Perceptron')\n",
    "\n",
    "ax = plt.subplot(1,3,2)\n",
    "plotBoundaries(w_logistic,Xtrain_i,ytrain)\n",
    "ax.set_title('Logistic Regression')\n",
    "\n",
    "ax = plt.subplot(1,3,3)\n",
    "plotBoundaries(w_svm,Xtrain_i,ytrain)\n",
    "ax.set_title('SVM')\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
